{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Decisão de Markov\n",
    "\n",
    "Modelos de Markov são **modelos estocásticos** usados para modelar sistemas pseudo-randômicos. \n",
    "Presume que os futuros estados só dependem do estado atual, não de eventos que o precederam (isso é a propriedade de Markov).\n",
    "\n",
    "Existem dois tipos de sistemas dentros desses modelos de Markov, modelos autonomos (onde a transição de um estado para o outro acontece ocasionalmente sem controle), e controlados onde podemos dizer que existe um **agente**.\n",
    "\n",
    "\n",
    "\n",
    "Dentro dessa categoria, os modelos de Markov se encaixam como Modelos de Decisão de Markov, que é definida pela quintupla:\n",
    "\n",
    "$$ M = \\{S, A, t, r, \\gamma\\}$$\n",
    "onde:\n",
    "- **S** é o conjunto de estados em um sistema.\n",
    "- **A** é o conjunto de ações.\n",
    "- **t** é a função de transição, que significa a probabilidade de chegar em um estado específico **s'** apartir de **s** tomando uma ação **a** na seguinte formatação: $$t:(s', s, a)->P$$, onde p varia de 0 a 1.\n",
    "- **r** é a recompensa por chegar em um estado s<sub>n</sub> na seguinte notação: $$r:(s_n, s_{n+1}, a)->r$$\n",
    "- **y** é o fator de desconto que determina quanto um agente valoriza recompensas futuras (tendendo a 1), ou recompensas imediatas (tendendo a 0).\n",
    "\n",
    "Outro conceito muito importante é a **policy**, que é a regra do modelo para selecionar determinadas ações dado um estado **s**.\n",
    "Por exemplo, em um modelo de xadrez, a **policy** seria a regra do modelo para selecionar uma certa ação dada uma posição do tabuleiro.\n",
    "A **policy** simplesmente escolhe as melhores ações para serem tomadas apartir de um estado **s** para maximizar a recompensa em um episódio.\n",
    "\n",
    "Existem diversas maneiras de ajustar a **policy**, mais o segredo para solucionar um determinado Modelo de Decisão de Markov é ajustar essa **policy** para um estado ideal (**optimal state**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Q-Learning\n",
    "\n",
    "Os algoritmos de Aprendizado por Reforço podem ser classificados em métodos baseados em **valor** ou **policy** (não tocaremos nesse segundo por enquanto).\n",
    "\n",
    "Métodos baseados em valores determinam uma **função de valor** que quantifica a recompensa. Usando essa função determinamos um **optimal state**.\n",
    "\n",
    "As funções de valores podem ser apenas baseadas no estado (**state-value function**), no formato: $$v:(s)->x \\in \\mathbb{R}$$\n",
    "Ou baseadas em estado-ação (state-action value function), no formato: $$Q:(s, a)>x \\in \\mathbb{R}$$.\n",
    "\n",
    "Esses **Q-values** são a base para o método de Q-learning, e é atualizada através da função de Bellman.\n",
    "\n",
    "$$ \\mathbb{Q(s, a)_{novo} = Q(s, a)_{antigo} + \\alpha * TDE(s, a)} $$\n",
    "\n",
    "Onde TDE(s,a) é a função de diferença temporal (obs: ver mais detalhes sobre).\n",
    "\n",
    "$$ \\mathbb{TDE(s, a) = Q(s, a)_{observado} - Q(s, a)_{antigo}} $$\n",
    "\n",
    "Onde Q(s, a)<sub>observado</sub> é calculado por:\n",
    "\n",
    "$$ \\mathbb{Q(s, a)_{observado} = r + \\gamma * max_a' Q(s_{posterior}, a')} $$\n",
    "\n",
    "Onde $$ \\mathbb{ max_a' Q(s_{posterior}, a')} $$ é o maior valor de **q-value** na tabela de **q-table**.\n",
    "\n",
    "Essa **q-table** é uma tabela que reflete os valor de **state-value function** atual de cada estado dado uma certa ação.\n",
    "\n",
    "Assim, o objetivo desse modelo é retornar uma **q-table** (que é atualizada a cada **step**) ideal, que refletea **policy ideal**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Então por exemplo, supondo um jogo com uma matriz 3x3 onde um robô tem que ou sair por uma porta ou cair em um buraco.\n",
    "Podemos denotar a matriz do jogo como: \n",
    "\n",
    "[M<sub>11</sub>, M<sub>12</sub>, M<sub>13</sub>]\n",
    "\n",
    "[M<sub>21</sub>, M<sub>22</sub>, M<sub>33</sub>]\n",
    "\n",
    "[M<sub>31</sub>, M<sub>32</sub>, M<sub>33</sub>]\n",
    "\n",
    "onde:\n",
    "- M<sub>11</sub> é onde o robô começa.\n",
    "- M<sub>32</sub> é onde o robô cai no buraco.\n",
    "- M<sub>33</sub> é onde o robô sai pela porta.\n",
    "\n",
    "Definindo-se as seguintes regras para esse jogo:\n",
    "- As recompensas em estados sem nada serão -1, no buraco -100 e na porta +100.\n",
    "- Um valor de y (é *épsilon*) de 0,9.\n",
    "- Um algoritmo **epsilon-greedy** para alternar entre **exploração** (nas etapas inicias) e **exploitation** (convergendo para política ótima).\n",
    "    - Vale ressaltar que por seguir inicialmente um método de **exploração** (ou seja ações randômicas), a **policy** alvo será diferente da abordagem que o agente usará (que é o epsilon-greedy) para alcançar essa **policy** alvo (por isso é **Q-learning** é chamado de **off-policy** diferentemente da SARSA por exemplo).\n",
    "- A q-table inicial sera iniciada com valores entre 0 e 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências\n",
    "\n",
    "Youtube: CodeEmperium\n",
    "Youtube: Steve Bruton\n",
    "Site: Geeks for Geeks\n",
    "Books: Reinforcement Learning for Finance\n",
    "LLM: DeepSeek"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
